# LocalChatBot-Using-Ollama
# Rishaud Richard, Student at Sheridan College

 This is my first project on github. With the new wave of AI, I decided to download a local LLM on my machine, and configure it using python to have my own chatbot that stores conversation memory per run, rather than storing all my inputs over time. This is useful for simple calculations, explanations etc instead of scouring google for a vague answer. Funny enough, the GUI version was developed by me, but the Ollama bot mainly assisted with me since i've never developed a GUI in python.

This project should be plug and play once you install Ollama, and the necessary dependencies. To set it up, I included documentation starting from scratch, but you could reference it if you download the files from this repo.


This project uses a Local LLM model from Ollama. With this, it allows the user to ask prompts to a local machine rather than an online accessible LLM such as ChatGPT
This is useful for simple calculations, explanations etc instead of scouring google for a vague answer
If you need any help with the project, you could visit Ollama's documentation, as well as emailing me at rishaudr14@gmail.com
Who maintains and contributes to the project
